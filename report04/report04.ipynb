{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "report04",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNb0d7e0Cgy0K81Tie3v+ZA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAeIBrwmKLHj"
      },
      "source": [
        "# report 04 2021.6.23\n",
        "import numpy as np\n",
        "from common.gradient import numerical_gradient\n",
        "from common.functions import sigmoid, softmax, sigmoid_grad\n",
        "from dataset.mnist import load_mnist\n",
        "def cross_entropy_error(y, t):\n",
        "   if y.ndim == 1:\n",
        "       t = t.reshape(1, t.size)\n",
        "       y = y.reshape(1, y.size)\n",
        "   # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
        "   if t.size == y.size:\n",
        "       t = t.argmax(axis=1)\n",
        "   batch_size = y.shape[0]\n",
        "   return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1PTFpzaNBHK"
      },
      "source": [
        "class SoftmaxWithLoss:\n",
        "   def __init__(self):\n",
        "       self.loss = None\n",
        "       self.y = None # softmaxの出力\n",
        "       self.t = None # 教師データ\n",
        "   def forward(self, x, t):\n",
        "       self.t = t\n",
        "       self.y = softmax(x)\n",
        "       # forwardの式\n",
        "       # -sum ( t * log (y))\n",
        "       self.loss = cross_entropy_error(self.y, self.t)\n",
        "       return self.loss\n",
        "   def backward(self, dout=1):\n",
        "       # backwardの式\n",
        "       # yi - ti (iはIndex)\n",
        "       batch_size = self.t.shape[0]\n",
        "       dx = ((self.y - self.t) / batch_size) ## TODO memo \n",
        "       return dx"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUmb1LX0Rq_T",
        "outputId": "9fba1038-4214-42c1-c49a-03b11c90822c"
      },
      "source": [
        "class TwoLayerNet:\n",
        "  def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers[\"Affine1\"] = Affine(self.params['W1'],self.params['b1'])\n",
        "    self.layers[\"Relu1\"] = ReLU()\n",
        "    self.layers[\"Affine2\"] = Affine(self.params['W2'],self.params['b2'])\n",
        "    self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return self.lastLayer.forward(y,t)\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    t = np.argmax(t, axis=1)\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "\n",
        "  def numerical_gradient(self, x, t):\n",
        "      loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "      grads = {}\n",
        "      grads[\"W1\"] = numerical_gradient(loss_W, self.params[\"W1\"])\n",
        "      grads[\"b1\"] = numerical_gradient(loss_W, self.params[\"b1\"])\n",
        "      grads[\"W2\"] = numerical_gradient(loss_W, self.params[\"W2\"])\n",
        "      grads[\"b2\"] = numerical_gradient(loss_W, self.params[\"b2\"])\n",
        "\n",
        "      return grads\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    self.loss(x,t)\n",
        "    dout = self.lastLayer.backward(1)\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers[\"Affine1\"].dW\n",
        "    grads['b1'] = self.layers[\"Affine1\"].db\n",
        "    grads['W2'] = self.layers[\"Affine2\"].dW\n",
        "    grads['b2'] = self.layers[\"Affine2\"].db\n",
        "    return grads\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W, self.b = W, b\n",
        "        self.x, self.dW, self.db = None, None, None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(x, self.W) + self.b\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        return dx\n",
        "\n",
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout.copy()\n",
        "        dx[self.mask] = 0\n",
        "        return dx\n",
        "\n",
        "import numpy as np\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "from dataset.mnist import load_mnist\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "# データの読み込み 公式サイトが503だったのでミラーの\n",
        "# https://storage.googleapis.com/cvdf-datasets/mnist/ に変更して実行\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "# 数値微分\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "# Backward\n",
        "#grad_backprop = gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "for key in grad_numerical.keys():\n",
        "   diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
        "   print(key + \":\" + str(diff))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1:2.8279509313595957e-13\n",
            "b1:1.0349251354990415e-12\n",
            "W2:1.0053347744707535e-12\n",
            "b2:1.1990409082285326e-10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZD2j81XafDx"
      },
      "source": [
        "## よくわからん"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-7KWRtcZTkI",
        "outputId": "e4387610-f8cf-447c-f1b5-7134d437bb01"
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "for key in grad_numerical.keys():\n",
        "   diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
        "   print(key + \":\" + str(diff))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1:1.1432583474035328e-06\n",
            "b1:1.2818677594115318e-05\n",
            "W2:1.0301085083942816e-12\n",
            "b2:1.1968204205459188e-10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUoIsS5mQwly"
      },
      "source": [
        "def gradient(network, x, t):\n",
        "   # 自分で実装したSoftmax with lossクラスを使ってみてください\n",
        "   lastLayer = SoftmaxWithLoss()\n",
        "   # forward\n",
        "   #self.loss(x, t)\n",
        "   network.loss(x, t)\n",
        "   # backward\n",
        "   dout = 1\n",
        "   dout = lastLayer.backward(dout)\n",
        "   #layers = list(self.layers.values())\n",
        "   layers = list(network.layers.values())\n",
        "   layers.reverse()\n",
        "   for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "   # 設定\n",
        "   grads = {}\n",
        "   #grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "   grads['W1'], grads['b1'] = network.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "   #grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "   grads['W2'], grads['b2'] = network.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "   return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGEDHxvOQzi5",
        "outputId": "41819245-8994-4bc9-a357-196ea9a9d8a8"
      },
      "source": [
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "# データの読み込み\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "for i in range(iters_num):\n",
        "   batch_mask = np.random.choice(train_size, batch_size)\n",
        "   x_batch = x_train[batch_mask]\n",
        "   t_batch = t_train[batch_mask]\n",
        "   # 勾配\n",
        "   #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "   #grad = gradient(x_batch, t_batch)\n",
        "   grad = network.gradient(x_batch, t_batch)\n",
        "   # 更新\n",
        "   for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "       network.params[key] -= learning_rate * grad[key]\n",
        "   loss = network.loss(x_batch, t_batch)\n",
        "   train_loss_list.append(loss)\n",
        "   if i % iter_per_epoch == 0:\n",
        "       train_acc = network.accuracy(x_train, t_train)\n",
        "       test_acc = network.accuracy(x_test, t_test)\n",
        "       train_acc_list.append(train_acc)\n",
        "       test_acc_list.append(test_acc)\n",
        "       print(train_acc, test_acc)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.07441666666666667 0.0723\n",
            "0.9053666666666667 0.9099\n",
            "0.9243833333333333 0.9267\n",
            "0.9328833333333333 0.9316\n",
            "0.9447333333333333 0.9426\n",
            "0.9513333333333334 0.948\n",
            "0.9565666666666667 0.9537\n",
            "0.96185 0.9568\n",
            "0.9635666666666667 0.9593\n",
            "0.96735 0.9637\n",
            "0.9699166666666666 0.9624\n",
            "0.97275 0.966\n",
            "0.9749833333333333 0.9687\n",
            "0.9759666666666666 0.9689\n",
            "0.9759 0.9672\n",
            "0.9788166666666667 0.97\n",
            "0.9789833333333333 0.9702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbb41oiFa4Uz"
      },
      "source": [
        "### なんもわからんかったので精進したい\n",
        "\n",
        "参考\n",
        "[GitHub1](https://github.com/statpng/DL-books/blob/cd1194c07338191b5058fe75fcbd9a2285c8331e/%5Bdeep-learning-from-scratch%5D_3.py)\n",
        "[GitHub検索](https://github.com/search?q=numerical_gradient%28&type=Code)"
      ]
    }
  ]
}